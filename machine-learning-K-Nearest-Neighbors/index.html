<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>【机器学习】k近邻算法 | 你刚刚打开了这个博客</title><meta name="keywords" content="machine-learning,knn"><meta name="author" content="Kevin Parker"><meta name="copyright" content="Kevin Parker"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="1 k近邻算法1.1 算法简介k近邻（K-Nearest Neighbors, KNN）法是一种基本分类与回归方法，支持多分类，于1968年由Cover和Hart提出。  输入：实例的特征空间向量。  输出：实例的类别。  算法思想：分类时，对新的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测。k近邻法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型”。  优点">
<meta property="og:type" content="article">
<meta property="og:title" content="【机器学习】k近邻算法">
<meta property="og:url" content="http://110.42.233.207/machine-learning-K-Nearest-Neighbors/index.html">
<meta property="og:site_name" content="你刚刚打开了这个博客">
<meta property="og:description" content="1 k近邻算法1.1 算法简介k近邻（K-Nearest Neighbors, KNN）法是一种基本分类与回归方法，支持多分类，于1968年由Cover和Hart提出。  输入：实例的特征空间向量。  输出：实例的类别。  算法思想：分类时，对新的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测。k近邻法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型”。  优点">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://110.42.233.207/machine-learning-K-Nearest-Neighbors/machine-learning.jpg">
<meta property="article:published_time" content="2021-11-27T12:05:35.000Z">
<meta property="article:modified_time" content="2021-12-17T08:18:39.959Z">
<meta property="article:author" content="Kevin Parker">
<meta property="article:tag" content="machine-learning">
<meta property="article:tag" content="knn">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://110.42.233.207/machine-learning-K-Nearest-Neighbors/machine-learning.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://110.42.233.207/machine-learning-K-Nearest-Neighbors/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '【机器学习】k近邻算法',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-12-17 16:18:39'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://avatars.githubusercontent.com/u/91741133?v=4" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">22</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">24</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-bookmark"></i><span> 文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/machine-learning-K-Nearest-Neighbors/machine-learning.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">你刚刚打开了这个博客</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-bookmark"></i><span> 文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">【机器学习】k近邻算法</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-11-27T12:05:35.000Z" title="发表于 2021-11-27 20:05:35">2021-11-27</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-12-17T08:18:39.959Z" title="更新于 2021-12-17 16:18:39">2021-12-17</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/machine-learning/">machine-learning</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="【机器学习】k近邻算法"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="1-k近邻算法"><a href="#1-k近邻算法" class="headerlink" title="1 k近邻算法"></a>1 k近邻算法</h1><h2 id="1-1-算法简介"><a href="#1-1-算法简介" class="headerlink" title="1.1 算法简介"></a>1.1 算法简介</h2><p><strong>k近邻（K-Nearest Neighbors, KNN）</strong>法是一种基本分类与回归方法，支持多分类，于1968年由Cover和Hart提出。</p>
<ul>
<li><p><strong>输入</strong>：实例的特征空间向量。</p>
</li>
<li><p><strong>输出</strong>：实例的类别。</p>
</li>
<li><p><strong>算法思想</strong>：分类时，对新的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测。k近邻法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型”。<br><img src="/machine-learning-K-Nearest-Neighbors/image-20211127201407994.png" alt="image-20211127201407994" style="zoom:50%;"></p>
</li>
<li><p><strong>优点</strong>：</p>
<ul>
<li>精度高</li>
<li>对异常值不敏感</li>
<li>无数据输入假定</li>
</ul>
</li>
<li><strong>缺点</strong>：<ul>
<li>计算复杂度高</li>
<li>空间复杂度高</li>
</ul>
</li>
<li><strong>适用范围</strong>：数值型、标称型</li>
</ul>
<p>k值的选择、距离度量方法及分类决策规则是k近邻法的三个要素。</p>
<h2 id="1-2-算法原理"><a href="#1-2-算法原理" class="headerlink" title="1.2 算法原理"></a>1.2 算法原理</h2><p>存在一个样本数据集合，也称作训练样本集 $X$ ，并且样本集中每个数据 $x_i$ 都存在标签 $y_i$ ，即样本集中每个数据与所属分类的对应关系。</p>
<p>输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签。</p>
<p>一般来说，只选择样本数据集中前N个最相似的数据。k一般不大于20，最后，选择k个中出现次数最多的分类，作为新数据的分类。</p>
<p><strong>完整算法</strong>如下：</p>
<ul>
<li>输入：数据集 $ T=\{(x_1,y_1),(x_2,y_2),…,(x_N,y_N)\} $ ，其中 $ x_i\in X\subseteq R^n $ 为实例特征向量， $ y_i\in Y=\{c_1,c_2,..,c_k\} $ 为实例类别， $ i=1,2,…,N $ 。</li>
<li>输出：实例 $x$ 所属类别 $y$ 。</li>
</ul>
<ol>
<li><p>根据给定的<u>距离度量</u>，在训练集 $T$ 中找出与 $x$ 最近邻的 k 个点，涵盖这 k 个点的 $x$ 的领域记作 $N_k(x)$ </p>
</li>
<li><p>在 $N_k(x)$ 中根据<u>分类决策规则</u>（如多数表决）决定 $x$ 的类别 $y$ ：</p>
<p> $y = arg\max_{c_j} \sum_{x_i \in N_k(x)} I(y_i=c_j) \ \ \ \ \ ,i=1,2,…,N;j=1,2,…,K$</p>
</li>
</ol>
<h3 id="1-2-1-k值的选择"><a href="#1-2-1-k值的选择" class="headerlink" title="1.2.1 k值的选择"></a>1.2.1 k值的选择</h3><ul>
<li><p>当 k=1 时，称为最近邻法，对于输入的实例点 $x$ ，最近邻法将训练数据集中与 $x$ 最近邻点的类作为 $x$ 的类。</p>
</li>
<li><p>当选择较小的K值时：</p>
<ul>
<li>“学习”的<u>近似误差</u>（approximation error)会减小，但“学习”的<u>估计误差</u>（estimation error) 会增大。</li>
<li>噪声敏感。</li>
<li>K值的<strong>减小</strong>，泛化能力就差，就意味着整体模型变得<strong>复杂</strong>，容易发生过拟合。</li>
</ul>
</li>
<li><p>当选择较大的K值时：</p>
<ul>
<li>减少学习的<u>估计误差</u>，但是学习的<u>近似误差</u>会增大。 </li>
<li>K值的<strong>增大</strong>，泛化能力就强，就意味着整体的模型变得<strong>简单</strong>。</li>
</ul>
</li>
</ul>
<h3 id="1-2-2-距离度量"><a href="#1-2-2-距离度量" class="headerlink" title="1.2.2 距离度量"></a>1.2.2 距离度量</h3><p>特征空间中两个实例点的距离是两个实例点相似程度的反映，有多种距离度量可选，一般选择 $L_p$ 距离（Minkowski距离）：</p>
<p>$L_p(x_i,x_j) = (\sum_{k=1}^n |x_i^{(k)} - x_j^{(k)}|^p)^{\frac{1}{p}} \ \ \ ,x\in R^n$</p>
<ul>
<li>曼哈顿距离，$p=1$ ：$L_1(x_i,x_j) = \sum_{k=1}^n |x_i^{(k)} - x_j^{(k)}|$</li>
<li>欧式距离， $p=2$ ： $L_2(x_i,x_j) = (\sum_{k=1}^n |x_i^{(k)} - x_j^{(k)}|^2)^{\frac{1}{2}}$</li>
<li>切比雪夫距离， $p=\infty$ ： $L_\infty(x_i,x_j) = \max_{k}|x_i^{(k)} - x_j^{(k)}|$ ，是各个维度距离中的最大值</li>
</ul>
<p>各距离度量中，与原点距离为1的点如下图所示：</p>
<p><img src="/machine-learning-K-Nearest-Neighbors/image-20211127205528363.png" alt="image-20211127205528363" style="zoom:50%;"></p>
<h3 id="1-2-3-分类决策规则"><a href="#1-2-3-分类决策规则" class="headerlink" title="1.2.3 分类决策规则"></a>1.2.3 分类决策规则</h3><p>对给定的实例 $x \in X$ ，其最近邻的 k 个训练实例点构成集合 $N_k(x)$ ，如果涵盖区域类别是 $c_j$ ，那么误分类概率是：</p>
<p>$\frac{1}{k} \sum_{x_i \in N_k(x)} I(y_i \ne c_j) = 1-\frac{1}{k} \sum_{x_i \in N_k(x)} I(y_i = c_j)$</p>
<p>要使误分类率最小，即经验风险最小，就要选 $c_j$ 使 $\sum_{x_i \in N_k(x)} I(y_i = c_j)$ 最大。</p>
<p>所以多数表决规则等价于经验最小化。</p>
<h1 id="2-kNN面临的问题与挑战"><a href="#2-kNN面临的问题与挑战" class="headerlink" title="2 kNN面临的问题与挑战"></a>2 kNN面临的问题与挑战</h1><h2 id="2-1-k值确定"><a href="#2-1-k值确定" class="headerlink" title="2.1 k值确定"></a>2.1 k值确定</h2><p>k值在准确率上是非单调，且一阶导也是非单调的，所以无法确定k值的优化方向（增大或是减少）。</p>
<p><img src="/machine-learning-K-Nearest-Neighbors/image-20211127210131967.png" alt="image-20211127210131967" style="zoom:35%;"></p>
<p>且k值太大太小都不合适，这一点在1.2.1小节中有提到。</p>
<h2 id="2-2-特征的选择"><a href="#2-2-特征的选择" class="headerlink" title="2.2 特征的选择"></a>2.2 特征的选择</h2><ul>
<li>不同特征有不同程度的影响，是否需要考虑在距离度量上对每个特征的影响加上权重，或者裁剪掉某些特征</li>
<li>有些特征不是连续的，可能是分类，之间的距离无法用数字来表示</li>
</ul>
<h2 id="2-3-距离函数的确定"><a href="#2-3-距离函数的确定" class="headerlink" title="2.3 距离函数的确定"></a>2.3 距离函数的确定</h2><p>距离计算方法有多种可以选择</p>
<h2 id="2-4-复杂度"><a href="#2-4-复杂度" class="headerlink" title="2.4 复杂度"></a>2.4 复杂度</h2><ul>
<li>需要计算新样本到所有训练数据的距离，时间和空间的复杂度都很高</li>
<li>时间和空间的复杂度与训练集大小相关</li>
</ul>
<h1 id="3-kd树"><a href="#3-kd树" class="headerlink" title="3 kd树"></a>3 kd树</h1><p>实现 k 近邻法时，主要考虑的问题是如何对训练数据进行快速 k 近邻搜索，在特征空间的维数大及训练数据容量大时尤其必要。</p>
<p>k 近邻法最简单的实现是线性扫描，这就需要输入实例与每个训练实例的距离，计算量很大。为提高搜索效率，可以考虑用特殊的结构存储训练数据，kd树方法是采用较多的一种。</p>
<h2 id="3-1-kd树构造算法"><a href="#3-1-kd树构造算法" class="headerlink" title="3.1 kd树构造算法"></a>3.1 kd树构造算法</h2><p><strong>完整算法</strong>如下：</p>
<ul>
<li>输入：k维空间数据集 $T=\{x_1,x_2,…,x_N\}\ \ ,x_i \in R^k \ \ ,i \in N$</li>
<li>输出：kd树</li>
</ul>
<ol>
<li>开始：构造根结点，根结点对应于包含<em>T</em>的<em>k</em>维空间的超矩形区域，<ol>
<li>选择 $x^{(1)}$ 方向为坐标轴，以 $T$ 中所有实例的 $x^{(1)}$ 坐标的中位数为切分点，将根结点对应的超矩形区域切分为两个子区域，切分由通过切分点并与坐标轴 $x^{(1)}$ 垂直的超平面实现，</li>
<li>由根结点生成深度为1的左、右子结点；左子结点对应坐标 $x^{(1)}$ 小于切分点的子区域，右子结点对应于坐标 $x^{(1)}$ 大于切分点的子区域，</li>
<li>将落在切分超平面上的实例点保存在根结点。</li>
</ol>
</li>
<li>重复：<ol>
<li>对深度为 $j$ 的结点，选择 $x^{(l)}$ 为切分的坐标轴，$l=j \mod k + 1$，以该结点的区域中所有实例的 $x^{(l)}$ 坐标的中位数为切分点，将该结点对应的超矩形分为两个子区域，切分由通过切分点并与坐标轴 $x^{(l)}$ 垂直的超平面实现。</li>
<li>由该结点生成深度为 $j+1$ 的左、右子结点，左子结点对应于坐标 $x^{(l)}$ 小于切分点的子区域，右子结点对应坐标 $x^{(l)}$ 大于切分点的子区域。</li>
<li>将落在切分超平面上的实例点保存在该结点。</li>
</ol>
</li>
<li>直到两个子区域没有实例点为止，从而形成kd树的区域划分。</li>
</ol>
<p><strong>举例</strong>如下：</p>
<p>二维空间的数据集 $T= \{(2,3)^T, (5,4)^T, (9,6)^T,(4,7)^T,(8,1)^T,(7,2)^T\}$</p>
<p>构造过程如下图所示：</p>
<p><img src="/machine-learning-K-Nearest-Neighbors/image-20211127213928739.png" alt="image-20211127213928739" style="zoom:50%;"><img src="/machine-learning-K-Nearest-Neighbors/image-20211127213937620.png" alt="image-20211127213937620" style="zoom:50%;"></p>
<h2 id="3-2-基于kd树的最近邻搜索算法"><a href="#3-2-基于kd树的最近邻搜索算法" class="headerlink" title="3.2 基于kd树的最近邻搜索算法"></a>3.2 基于kd树的最近邻搜索算法</h2><p><strong>完整算法</strong>如下：</p>
<ul>
<li>输入：已经构造的kd树，目标点 $x$</li>
<li>输出：$x$ 的最近邻</li>
</ul>
<ol>
<li><p>从根到叶：</p>
<p> 在kd树中找出包含目标点 $x$ 的叶结点：从根结点出发，递归地向下访问kd树。若目标点 $x$ 当前维的坐标小于切分点的坐标，则移动到左子结点，否则移动到右子结点。直到叶结点为止。</p>
</li>
<li><p>从叶向根：</p>
<p> 以此叶结点为 “当前最近点” ，递归地向上回退，在每个父结点进行以下操作：</p>
<ol>
<li>如果该结点保存的实例点比当前最近点距离目标更近，则以该实例点为 “当前最近点” </li>
<li>“当前最近点” 一定存在于该结点一个子结点对应的区域。检查该子结点的父结点的另一子结点对应的区域是否有更近的点。即检查另一子结点对应的区域是否与 “以目标点位球心，以目标点与“当前最近点”间的距离为半径的超球体” 相交。<ul>
<li>如果相交，可能在另一子节点的区域内存在距目标点更近的点，移动到另一子节点。接着递归地进行最近邻搜索。</li>
<li>如果不相交，向上回退。</li>
</ul>
</li>
</ol>
</li>
<li><p>当回退到根结点时，搜索结束，最后的“当前最近点”即为x的最近邻点。</p>
</li>
</ol>
<p>kd树的一些<strong>性质</strong>：</p>
<ul>
<li><p>如果实例点是随机分布的，kd树搜索的平均计算复杂度是 $O(logN)$。</p>
</li>
<li><p>kd树更适合用于训练实例数远大于空间维数的k近邻搜索。当空间维数接近训练实例数时，kd树搜索会退化为顺序扫描。</p>
</li>
</ul>
<h1 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h1><p><a href="/machine-learning-exp-KNN">【机器学习】使用k近邻算法及其变种预测的糖尿病实践</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Kevin Parker</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://110.42.233.207/machine-learning-K-Nearest-Neighbors/">http://110.42.233.207/machine-learning-K-Nearest-Neighbors/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://110.42.233.207" target="_blank">你刚刚打开了这个博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/machine-learning/">machine-learning</a><a class="post-meta__tags" href="/tags/knn/">knn</a></div><div class="post_share"><div class="social-share" data-image="/machine-learning-K-Nearest-Neighbors/machine-learning.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/machine-learning-Naive-Bayesian-Model/"><img class="prev-cover" src="/machine-learning-Naive-Bayesian-Model/machine-learning.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">【机器学习】朴素贝叶斯</div></div></a></div><div class="next-post pull-right"><a href="/machine-learning-Perceptron/"><img class="next-cover" src="/machine-learning-Perceptron/machine-learning.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">【机器学习】感知机</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/machine-learning-exp-KNN/" title="【机器学习】实践-使用k近邻算法及其变种预测的糖尿病"><img class="cover" src="/machine-learning-exp-KNN/machine-learning.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-05</div><div class="title">【机器学习】实践-使用k近邻算法及其变种预测的糖尿病</div></div></a></div><div><a href="/machine-learning-Decision-Tree/" title="【机器学习】决策树"><img class="cover" src="/machine-learning-Decision-Tree/machine-learning.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-28</div><div class="title">【机器学习】决策树</div></div></a></div><div><a href="/machine-learning-LogisticRegression/" title="【机器学习】Logistic回归"><img class="cover" src="/machine-learning-LogisticRegression/machine-learning.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-03</div><div class="title">【机器学习】Logistic回归</div></div></a></div><div><a href="/machine-learning-Naive-Bayesian-Model/" title="【机器学习】朴素贝叶斯"><img class="cover" src="/machine-learning-Naive-Bayesian-Model/machine-learning.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-28</div><div class="title">【机器学习】朴素贝叶斯</div></div></a></div><div><a href="/machine-learning-MaxEntropyModel/" title="【机器学习】最大熵模型"><img class="cover" src="/machine-learning-MaxEntropyModel/machine-learning.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-04</div><div class="title">【机器学习】最大熵模型</div></div></a></div><div><a href="/machine-learning-Perceptron/" title="【机器学习】感知机"><img class="cover" src="/machine-learning-Perceptron/machine-learning.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-27</div><div class="title">【机器学习】感知机</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://avatars.githubusercontent.com/u/91741133?v=4" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Kevin Parker</div><div class="author-info__description">一只菜鸟的学习笔记</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">22</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">24</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Kairan-Peng"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Kairan-Peng" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:peng.kairan@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95"><span class="toc-text">1 k近邻算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-%E7%AE%97%E6%B3%95%E7%AE%80%E4%BB%8B"><span class="toc-text">1.1 算法简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86"><span class="toc-text">1.2 算法原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-1-k%E5%80%BC%E7%9A%84%E9%80%89%E6%8B%A9"><span class="toc-text">1.2.1 k值的选择</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-2-%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F"><span class="toc-text">1.2.2 距离度量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-3-%E5%88%86%E7%B1%BB%E5%86%B3%E7%AD%96%E8%A7%84%E5%88%99"><span class="toc-text">1.2.3 分类决策规则</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-kNN%E9%9D%A2%E4%B8%B4%E7%9A%84%E9%97%AE%E9%A2%98%E4%B8%8E%E6%8C%91%E6%88%98"><span class="toc-text">2 kNN面临的问题与挑战</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-k%E5%80%BC%E7%A1%AE%E5%AE%9A"><span class="toc-text">2.1 k值确定</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-%E7%89%B9%E5%BE%81%E7%9A%84%E9%80%89%E6%8B%A9"><span class="toc-text">2.2 特征的选择</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-%E8%B7%9D%E7%A6%BB%E5%87%BD%E6%95%B0%E7%9A%84%E7%A1%AE%E5%AE%9A"><span class="toc-text">2.3 距离函数的确定</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-%E5%A4%8D%E6%9D%82%E5%BA%A6"><span class="toc-text">2.4 复杂度</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-kd%E6%A0%91"><span class="toc-text">3 kd树</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-kd%E6%A0%91%E6%9E%84%E9%80%A0%E7%AE%97%E6%B3%95"><span class="toc-text">3.1 kd树构造算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-%E5%9F%BA%E4%BA%8Ekd%E6%A0%91%E7%9A%84%E6%9C%80%E8%BF%91%E9%82%BB%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95"><span class="toc-text">3.2 基于kd树的最近邻搜索算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E6%96%87%E7%AB%A0"><span class="toc-text">相关文章</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/machine-learning-ensemble-learning/" title="【机器学习】提升算法"><img src="/machine-learning-ensemble-learning/machine-learning.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【机器学习】提升算法"/></a><div class="content"><a class="title" href="/machine-learning-ensemble-learning/" title="【机器学习】提升算法">【机器学习】提升算法</a><time datetime="2022-01-10T13:33:00.000Z" title="发表于 2022-01-10 21:33:00">2022-01-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/cv-exp-color-shape-recognisation/" title="【计算机视觉】图像颜色和形状识别实验"><img src="/cv-exp-color-shape-recognisation/img.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【计算机视觉】图像颜色和形状识别实验"/></a><div class="content"><a class="title" href="/cv-exp-color-shape-recognisation/" title="【计算机视觉】图像颜色和形状识别实验">【计算机视觉】图像颜色和形状识别实验</a><time datetime="2021-12-22T13:13:23.000Z" title="发表于 2021-12-22 21:13:23">2021-12-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/remote-ssh/" title="【SSH】使用ssh连接个人win10电脑"><img src="/remote-ssh/ssh.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【SSH】使用ssh连接个人win10电脑"/></a><div class="content"><a class="title" href="/remote-ssh/" title="【SSH】使用ssh连接个人win10电脑">【SSH】使用ssh连接个人win10电脑</a><time datetime="2021-12-18T04:58:50.000Z" title="发表于 2021-12-18 12:58:50">2021-12-18</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By Kevin Parker</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="icp"><a target="_blank" rel="noopener" href="http://beian.miit.gov.cn/"><span>浙ICP备2021037349号-1</span></a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>